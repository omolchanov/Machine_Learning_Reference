# https://ranasinghiitkgp.medium.com/feature-engineering-using-featuretools-with-code-10f8c83e5f68

import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_percentage_error

from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from catboost import CatBoostRegressor

import pandas as pd
import numpy as np
import featuretools as ft

from scipy.stats import chi2_contingency
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Configuration
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 1)
pd.set_option('display.float_format', lambda x: '%.3f' % x)
np.set_printoptions(threshold=np.inf, suppress=True)

df_train = pd.read_csv('../assets/big_mart_sales_train.csv')
df_test = pd.read_csv('../assets/big_mart_sales_test.csv')

# Appending train and test dataframes
df = df_train._append(df_test, ignore_index=True)

# Dealing with missing values
df['Item_Weight'].fillna(df['Item_Weight'].mean(), inplace=True)
df['Item_Outlet_Sales'].fillna(df['Item_Outlet_Sales'].mean(), inplace=True)
df['Outlet_Size'].fillna('missing', inplace=True)


# print('NaN values:\n', df.isnull().sum(axis=0))


def prepare_former_df(former_df):
    # Removing unused columns
    new_df = former_df.drop(['Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales'], axis=1)

    # Encoding 'Item_Fat_Content' feature to numerical
    new_df['Item_Fat_Content'] = new_df['Item_Fat_Content'].replace({
        'Low Fat': 0,
        'Regular': 1,
        'LF': 0,
        'reg': 1,
        'low fat': 0
    }, regex=True)

    # Encoding categorical features to numerical
    new_df['Item_Type'] = LabelEncoder().fit_transform(new_df['Item_Type'])
    new_df['Outlet_Size'] = LabelEncoder().fit_transform(new_df['Outlet_Size'])
    new_df['Outlet_Location_Type'] = LabelEncoder().fit_transform(new_df['Outlet_Location_Type'])
    new_df['Outlet_Type'] = LabelEncoder().fit_transform(new_df['Outlet_Type'])

    # print(new_df.info())
    # print(new_df.head(10))

    return new_df


def prepare_ext_df(ext_df):
    new_df = ext_df.drop([
        'id',
        'Outlet_Identifier',
        'Item_Identifier',
        'outlet.MODE(bigmart.Item_Identifier)',
        'outlet.MODE(bigmart.Item_Fat_Content)',
    ], axis=1)

    # Encoding 'Item_Fat_Content' feature to numerical
    new_df['Item_Fat_Content'] = new_df['Item_Fat_Content'].replace({
        'Low Fat': 0,
        'Regular': 1,
        'LF': 0,
        'reg': 1,
        'low fat': 0
    }, regex=True)

    # Encoding categorical features to numerical
    new_df['Item_Type'] = LabelEncoder().fit_transform(new_df['Item_Type'])
    new_df['outlet.Outlet_Size'] = LabelEncoder().fit_transform(new_df['outlet.Outlet_Size'])
    new_df['outlet.Outlet_Location_Type'] = LabelEncoder().fit_transform(new_df['outlet.Outlet_Location_Type'])
    new_df['outlet.Outlet_Type'] = LabelEncoder().fit_transform(new_df['outlet.Outlet_Type'])

    new_df['outlet.MODE(bigmart.Item_Type)'] = LabelEncoder().fit_transform(new_df['outlet.MODE(bigmart.Item_Type)'])

    # print(new_df.info())
    # print(new_df.head(10))

    return new_df


def perform_regression(df, reg):
    X = df.drop('Item_MRP', axis=1)
    y = df['Item_MRP']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    reg.fit(X_train, y_train)

    y_pred = reg.predict(X_test)

    print('\n', reg)
    print('R2: %.3f' % (r2_score(y_pred, y_test)))
    print('MAPE: %.2f' % (mean_absolute_percentage_error(y_pred, y_test) * 100))
    print('==========================')


def compose_new_features():
    # Composing a unique ID for feature tools
    df['id'] = df['Item_Identifier'] + df['Outlet_Identifier']

    # Creating a Featuretools identity set, creating relationships
    es = ft.EntitySet(id='sales')
    es.add_dataframe(dataframe_name='bigmart', dataframe=df, index='id')

    es.normalize_dataframe(
        base_dataframe_name='bigmart',
        index='Outlet_Identifier',
        new_dataframe_name='outlet',
        additional_columns=['Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']
    )

    # Using Deep Feature Synthesis to create new features automatically
    # The parameter max_depth controls the complexity of the features being generated by stacking the primitives
    feature_matrix, feature_names = ft.dfs(
        entityset=es,
        target_dataframe_name='bigmart',
        max_depth=2,
        verbose=1,
        n_jobs=1
    )
    feature_matrix = feature_matrix.reindex(index=df['id'])
    ext_df = feature_matrix.reset_index()

    # Printing the result: dataframe with new features
    # print(df.head(10))
    # print(ext_df.head(10))

    return ext_df


def analyze_df(df):
    print(df.corr())

    for _, c in enumerate(df.columns):
        contingency_table = pd.crosstab(df['Item_MRP'], df[c])
        chi2_value, p, _, _ = chi2_contingency(contingency_table)

        print('P-value of ', c, ':', p)

    try:
        vif = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
        result = pd.DataFrame({'Variance Inflation Factor': vif[0:]}, index=df.columns).T
        print('\n', result)
    except Exception:
        print('VIF can not be performed')


def run():
    regs = [
        CatBoostRegressor(
            iterations=100,
            learning_rate=0.3,
            depth=6,
            random_seed=7,
            verbose=0,
            cat_features=['Item_Fat_Content']
        ),
        DecisionTreeRegressor(),
        LinearRegression(),
        Lasso(),
        Ridge()
    ]

    print('==== EVALUATING FORMER DF =====')
    former_df = df
    pr_df = prepare_former_df(former_df)

    for _, reg in enumerate(regs):
        perform_regression(pr_df, reg)

    print('==== EVALUATING EXTENDED DF =====')
    ext_df = compose_new_features()
    pr_df = prepare_ext_df(ext_df)

    for _, reg in enumerate(regs):
        perform_regression(pr_df, reg)


def analyze():
    print('============ANALYSIS OF FORMER DF===============')
    former_df = df
    analyze_df(prepare_former_df(former_df))

    print('============ANALYSIS OF EXTENDED DF===============')
    ext_df = compose_new_features()
    analyze_df(prepare_ext_df(ext_df))


if __name__ == '__main__':
    # analyze()
    run()

